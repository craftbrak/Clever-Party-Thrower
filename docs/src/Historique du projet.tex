\subsection{Introduction}\label{subsec:introduction2}
Dans cette section, nous examinerons l'évolution du projet, depuis sa phase de conceptualisation jusqu'à sa réalisation finale, en soulignant les différentes difficultés rencontrées en cours de route.
Nous mettrons également en lumière les jalons clés du projet, les ajustements de la planification et les interactions avec le rapporteur.

\subsection{Chronologie du projet}\label{subsec:chronologie-du-projet}
Le projet a officiellement débuté en septembre 2022, marqué par une première phase d'échanges intenses avec de potentiels utilisateurs et de définition des objectifs.
Des étapes clés ont jalonné notre progression, telles que la finalisation du cahier des charges et la sélection des technologies en octobre 2022,
le commencement du développement dans la même période, et enfin, le déploiement de la premiere version l'application en juin 2023.

\subsection{Réunions et interactions avec le client/rapporteur}\label{subsec:reunions-et-interactions-avec-le-client/rapporteur}
Au cours du projet, j'ai eu plusieurs réunions avec mon rapporteur.
Ces réunions ont été l'occasion de discuter de l'avancement du projet, de recueillir des retours constructifs et d'adapter notre approche en conséquence.
Par exemple, lors de la réunion de validation du sujet en octobre, nous avons convenu que la conception de l'application
devrait être suffisamment flexible pour permettre l'ajout de nouvelles fonctionnalités à l'avenir, comme un système de covoiturage.

\subsection{Évolution des choix techniques et stratégiques}\label{subsec:evolution-des-choix-techniques-et-strategiques}
Au fil du projet, certaines modifications ont été nécessaires.
Ces ajustements étaient principalement dus à RxJS. Initialement, j'avais prévu de concevoir le projet de manière à ce que l'application soit réactive sur l'ensemble du stack.
Cependant, après une discussion éclairante avec mon rapporteur, M. Noel,
nous avons décidé de construire le projet de manière classique tout en envisageant l'ajout de la réactivité dans une phase future.

\subsection{Defits lors du developement}\label{subsec:defits-lors-du-developement}
Lors du develepement du projets plusieurs defit on du etre surmonter

\subsubsection{L'algorithme des dettes}
Le développement d'un algorithme capable de répartir les différents coûts d'un événement entre les utilisateurs s'est avéré être un défi.
Non seulement il fallait que les différentes dépenses existantes à un instant T soient réparties entre les différents participants, mais il fallait aussi permettre aux participants de continuer à ajouter des dépenses, même une fois qu'un utilisateur a remboursé ses dettes.

Au départ, l'algorithme était plutôt simple : récupérer toutes les dépenses pour un événement donné dans la base de données, calculer le total des dépenses pour l'événement et créer une dette par participant, dont la valeur est le total divisé par le nombre de participants.

Cependant, cet algorithme n'était pas satisfaisant pour plusieurs raisons, principalement le manque de possibilité de spécifier à qui les dettes doivent être remboursées.
J'ai donc décidé de changer d'approche.
Au lieu de calculer le total des dépenses pour un événement, nous allons calculer la balance totale de chaque participant.

Pour cela, lors du calcul des dettes, on crée une carte associant un participant à une balance.
Une fois que la balance de chaque participant est créée sur la base de toutes les dépenses, on parcourt cette carte afin de créer des paires de participants.
L'objectif est de trouver le participant avec la balance la plus positive et celui avec la balance la plus négative.
Une fois que l'on a cette paire, on peut créer une dette entre ces deux utilisateurs de sorte à ce que l'un ou les deux voient leur balance revenir à zéro.
Après avoir créé cette dette, on met à jour la balance des deux utilisateurs.
On continue ainsi jusqu'à ce que toutes les balances des utilisateurs valent zéro

À première vue, cette stratégie fonctionne.
Cependant, comme je l'ai découvert plus tard dans le développement, elle présente plusieurs problèmes majeurs.
Premièrement, cet algorithme ne permet pas aux utilisateurs de marquer une dette comme remboursée et que cela persiste au travers des différents calculs.
Deuxièmement, les participants ne peuvent pas créer de dépenses entre eux et donc un participant qui aurait par exemple participé à un covoiturage ne paierait pas plus qu'un participant qui se serait déplacé par ses propres moyens.
Troisièmement, une dépense est obligatoirement répartie entre tous les participants.

Pour remédier à ces problèmes, il a fallu modifier les dépenses afin de permettre de sélectionner un acheteur et un bénéficiaire.
Désormais, chaque achat dans la liste des courses, par exemple, crée une dépense entre l'acheteur et chacun des bénéficiaires.
Un champ permettant de conserver la balance d'un utilisateur a été ajouté à l'entité EventToUser afin d'optimiser les calculs (plus besoin de parcourir toutes les dépenses pour pouvoir générer les dettes).
Pour gérer le remboursement des dettes, nous profitons de l'ajout d'un acheteur à une dépense.
Lorsqu'un participant marque une dette comme remboursée, une dépense de valeur opposée est créée entre le créancier et le débiteur afin de contrebalancer la dette.
Cette dernière est alors supprimée.

La solution mise en place permet de gérer efficacement le remboursement des dettes et de répartir les dépenses parmi les consommateurs,
optimisant ainsi les calculs de remboursement.
Grâce à une modification récente, les participants peuvent désormais diviser une dépense de manière inégale ou choisir de ne pas la répartir entre tous les participants,
ajoutant une couche supplémentaire de flexibilité à cet algorithme.

\subsubsection{le cluster kubernetees}
La conception et la mise en place d'un cluster Kubernetes ont probablement été les défis majeurs de ce projet.
L'apprentissage des différents outils, la conception du cluster et son déploiement ont été des étapes cruciales.
Au début du projet, j'étais totalement novice avec Kubernetes et Ansible n'était pour moi qu'un nom.

J'ai donc dû m'initier au fonctionnement de Kubernetes et aux différents outils pouvant être utilisés en parallèle.
Mes recherches m'ont conduit à découvrir une immense communauté de passionnés qui ont appris à utiliser Kubernetes
et ont créé de la documentation à ce sujet pour le plaisir ou pour des fins professionnelles.
Techno Tim, l'un d'entre eux, a créé une série de vidéos où il partage les différentes configurations mises en place
dans son micro datacenter qu'il gère chez lui.
C'est en découvrant ces vidéos que j'ai commencé à apprendre comment configurer Kubernetes et utiliser Ansible.

Grâce à ces nouvelles connaissances, j'ai décidé de mettre en place sur l'un de mes serveurs un cluster qui,
grâce à la virtualisation, serait hautement disponible.
En réalité, seule la redondance du cluster Kubernetes est intégrée ;
il n'y a pas de redondance au niveau matériel car je n'avais pas les moyens de l'implémenter.

J'ai donc commencé par créer des machines virtuelles (VM) sur mon hyperviseur (Proxmox).
Après une mauvaise manipulation, j'ai décidé de recommencer depuis le début.
Je me suis alors rendu compte que créer les VM manuellement n'était pas l'idéal,
mais je ne voulais pas mettre en place un système de MaaS (Metal as a Service) qui aurait trop sollicité mon serveur.
J'ai donc choisi une solution intermédiaire : je n'ai pas complètement automatisé la configuration et le provisionnement
de machines virtuelles, mais j'ai cherché à simplifier considérablement le processus.

C'est alors que j'ai découvert deux outils formidables : les templates de VM qemu et CloudInit.
Grâce à ces outils, je pouvais facilement créer des machines virtuelles avec mes paramètres,
et modifier ceux-ci via CloudInit avant même de démarrer la machine virtuelle.

Fort de cette nouvelle découverte, j'ai mis en place sept VM dédiées au cluster Kubernetes :
trois serviront de plans de contrôle et quatre de nœuds d'exécution.

Le projet nécessitant une infrastructure d'hébergement et étant conteneurisé, tout s'alignait parfaitement.

Après des jours d'expérimentation manuelle avec Kubernetes via kubectl, Lens et Portainer, j'ai décidé de ne plus vouloir
d'un cluster unique et fragile comme un flocon de neige.
Je me suis tourné vers Ansible, un outil dont j'avais entendu beaucoup de bien, afin de définir mon infrastructure en tant que code.

J'ai cherché comment configurer un cluster Kubernetes via Ansible et j'ai découvert l'existence d'une communauté de passionnés
qui ont créé et publié en open-source des playbooks Ansible complets permettant de mettre en place un cluster.

Dans cette communauté, j'ai retrouvé Techno Tim qui a, lui aussi, contribué en adaptant un playbook existant
afin de déployer un cluster hautement disponible.
J'ai donc décidé de partir de son projet pour construire mon infrastructure.
J'ai ajouté au script original cert-manager, Traefik, Rancher, Longhorn et ArgoCD\@.

L'ajout de cert-manager et Traefik a été plutôt simple, mais l'intégration de Rancher s'est avérée plus complexe pour plusieurs raisons.

Premièrement, mon serveur était très limité en ressources pour ce type de tâche, encore plus à cause de la virtualisation
et du niveau de redondance nécessaire à la haute disponibilité.
Deuxièmement, la version de Rancher que j'essayais d'installer à ce moment-là n'était tout simplement pas compatible avec K3S
(l'implémentation de Kubernetes que j'avais décidé de déployer).
Enfin, après avoir changé de version de Rancher et sélectionné une version compatible avec K3S,
je me suis rendu compte que cette version n'était pas compatible avec la version spécifique de K3S que j'avais choisie.
J'ai donc dû changer une fois de plus de version de Rancher.
J'ai finalement pu finaliser la mise en place de mon cluster.

Deux mois plus tard, j'ai voulu apporter une modification au cluster, plutôt simple : j'ai voulu ajouter un certificat TLS.
Au lieu de simplement modifier le cluster existant, j'ai décidé de tirer profit de mon IaC pour pérenniser ce changement.
J'ai donc modifié les détails dans le playbook et je l'ai lancé.
Malheureusement, le cluster ne répondait plus\ldots

En fait, en raison d'un manque chronique de ressources, certains processus essentiels
à Kubernetes n'avaient pas pu être exécutés à temps, ce qui a entraîné l'arrêt de son fonctionnement.
J'ai donc décidé de recréer le cluster à partir de zéro.
Cependant, mon playbook, qui deux mois plus tôt fonctionnait parfaitement, ne fonctionnait plus du tout.

Après de longues heures de débogage, je n'arrivais pas à trouver la cause du problème,
même après être revenu à la dernière version du playbook qui avait servi à déployer le cluster.
Impossible de redéployer le cluster.
Rien n'avait changé dans le reste de l'infrastructure pourtant.

Le playbook affichait toujours la même erreur : "helm: jetstack repository not found". %todo: change error message for the real one
Le problème est que la commande d'ajout de ce référentiel est bien exécutée et sans erreurs et que lorsque je teste manuellement,
le référentiel est bien ajouté à Helm et disponible.

J'ai consacré de nombreuses heures à essayer de résoudre ce problème, mais sans succès.
C'était frustrant et déroutant, car je n'avais apporté aucun changement majeur au playbook depuis son dernier déploiement réussi.

Face à ce blocage, j'ai été contraint de reconsidérer l'approche technique pour le déploiement de mon projet.
J'ai donc décidé de mettre Kubernetes de côté et de me concentrer sur Docker-compose.
Bien que ce ne soit pas la solution initiale envisagée,
Docker-compose m'a permis de poursuivre le développement du projet tout en conservant l'aspect conteneurisé de l'application.

\paragraph{Le choix de Docker-compose}


Docker-compose s'est révélé être une alternative efficace à Kubernetes, grâce à sa simplicité d'utilisation et à sa moindre consommation de ressources.
Il m'a permis de définir et de gérer plusieurs conteneurs comme un ensemble de services interconnectés, ce qui correspondait parfaitement aux besoins de mon projet.
De plus, la préparation d'un fichier docker-compose était prévue dans le cadre du projet pour accompagner les clients
potentiels qui ne seraient pas en mesure de déployer le projet sur Kubernetes mais qui pourraient aisément l'exploiter via Docker-compose.

La migration vers Docker-compose s'est faite sans difficultés majeures, étant donné ma familiarité avec Docker et le fait que mon application était déjà conteneurisée.
Quelques ajustements dans le fichier docker-compose.yml ont été suffisants pour rendre mon application pleinement opérationnelle.

Ce revirement de situation m'a libéré du temps pour me concentrer sur d'autres aspects du projet,
tels que le développement de nouvelles fonctionnalités et l'amélioration de l'expérience utilisateur.
Malgré une certaine déception de ne pas avoir pu implémenter Kubernetes comme initialement prévu, je reste satisfait du résultat final.
Docker-compose a parfaitement répondu à mes besoins et m'a permis de mener à bien mon projet.

Pour conclure, bien que la tentative d'implémentation de Kubernetes ait été un défi majeur qui n'a pas abouti comme je l'espérais,
ce projet reste une riche expérience d'apprentissage.
J'ai pu approfondir ma compréhension des outils de déploiement et d'orchestration de conteneurs et développer des compétences précieuses en matière de débogage et de résolution de problèmes.
Malgré les difficultés rencontrées, je suis fier du travail accompli et je reste déterminé à explorer davantage Kubernetes dans le cadre de futurs projets.

\subsubsection{le client graphql}

Comme, je l'ai mentionné dans les sections précédentes, le développement du front-end a été réalisé à l'aide d'Angular.
Ce dernier doit interagir avec le back-end par le biais d'une API GraphQL. C'est pour cela que j'ai choisi d'utiliser Apollo Angular.

Pour configurer Apollo Angular, il est nécessaire de créer un fichier graphql.module.ts contenant les configurations minimales recommandées par la documentation.
Afin de permettre l'authentification auprès du back-end via JWT, un header doit être configuré avec le token.

Cependant, j'ai rencontré un problème : le token est géré par l'un de mes services et la configuration d'Apollo se fait hors du champ d'action de l'injection de dépendance d'Angular.
De plus, en cas d'erreur de requête, je souhaitais pouvoir capturer cette erreur et agir différemment en fonction du message reçu.
Par exemple, une erreur 400 devrait simplement être consignée dans la console, tandis qu'une erreur 401 devrait rediriger l'utilisateur vers la page de login.

Malheureusement, je me trouvais dans l'impossibilité de récupérer mes tokens et de rediriger mes utilisateurs vers la page appropriée grâce au routeur Angular.

Finalement, j'ai contourné ces problèmes en utilisant directement les éléments JavaScript (window) et le local storage.
Cela m'a permis de récupérer le token et de gérer les redirections adéquates lors des erreurs.
